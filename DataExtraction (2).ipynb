{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jSIeKXphKNhp",
        "outputId": "0d54d7d9-e080-496c-b97b-cbba85fa06fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Installing collected packages: python-docx, vaderSentiment, newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7 python-docx-1.1.2 vaderSentiment-3.3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles...\n",
            "Articles fetched successfully.\n",
            "Fetching article content...\n",
            "Filtered DataFrame has 85 rows.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d1d6b346c9e5>:178: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['sentiment_score'] = df_filtered['full_content'].apply(analyze_sentiment)\n",
            "<ipython-input-1-d1d6b346c9e5>:179: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['party'] = df_filtered['full_content'].apply(determine_main_party_weighted)\n",
            "<ipython-input-1-d1d6b346c9e5>:180: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['sentiment_label'] = df_filtered['sentiment_score'].apply(get_sentiment_label)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis completed for 59 out of 85 rows with identified parties.\n",
            "Excel file found. Attempting to download...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cb328c54-2e0f-4a05-9531-42b285ab850d\", \"output.xlsx\", 232757)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file with full content found.\n",
            "Preview of saved CSV:\n",
            "                                              source  \\\n",
            "0  {'id': 'business-insider', 'name': 'Business I...   \n",
            "1                     {'id': 'time', 'name': 'Time'}   \n",
            "2  {'id': 'business-insider', 'name': 'Business I...   \n",
            "3  {'id': 'business-insider', 'name': 'Business I...   \n",
            "4        {'id': None, 'name': 'Yahoo Entertainment'}   \n",
            "\n",
            "                                              author  \\\n",
            "0                                     Rebecca Rommen   \n",
            "1                                    Astha Rajvanshi   \n",
            "2                                        Matthew Loh   \n",
            "3                                         Tom Porter   \n",
            "4  Vandinika Shukla, Harvard Kennedy School and B...   \n",
            "\n",
            "                                               title  \\\n",
            "0  Indian authorities seize over $1 billion worth...   \n",
            "1  The Controversy Over a New Population Study Fr...   \n",
            "2  It's been 3 days since Modi won, and we're alr...   \n",
            "3  The competition between India and China is abo...   \n",
            "4  Indian election was awash in deepfakes – but A...   \n",
            "\n",
            "                                         description  \\\n",
            "0  The winner of India's general election, which ...   \n",
            "1  Critics say the study could sow communal disco...   \n",
            "2  Modi's next term as prime minister is set to l...   \n",
            "3  India and China are locked in a battle between...   \n",
            "4  Campaigns used deepfakes to connect with voter...   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.businessinsider.com/indian-authori...   \n",
            "1  https://time.com/6976854/india-population-stud...   \n",
            "2  https://www.businessinsider.com/narendra-modi-...   \n",
            "3  https://www.businessinsider.com/india-china-co...   \n",
            "4  https://www.yahoo.com/news/indian-election-awa...   \n",
            "\n",
            "                                          urlToImage           publishedAt  \\\n",
            "0  https://i.insider.com/6649a39d20abc1efe8fb6473...  2024-05-19T13:22:38Z   \n",
            "1  https://api.time.com/wp-content/uploads/2024/0...  2024-05-10T15:16:47Z   \n",
            "2  https://i.insider.com/666286961cd3b17790443e58...  2024-06-07T05:59:02Z   \n",
            "3  https://i.insider.com/66631e09cc442a2f676f130d...  2024-06-08T10:03:17Z   \n",
            "4  https://s.yimg.com/ny/api/res/1.2/JhjdV20_cCeY...  2024-06-10T13:43:43Z   \n",
            "\n",
            "                                             content  \\\n",
            "0  Election duty staff cast their votes on May 13...   \n",
            "1  A working paper from an independent body that ...   \n",
            "2  India's Prime Minister Narendra Modi addresses...   \n",
            "3  Jack Taylor/Getty Images; Carlos Barria/Pool/A...   \n",
            "4  As India concluded the worlds largest election...   \n",
            "\n",
            "                                        full_content  sentiment_score party  \\\n",
            "0  The Election Commission of India (ECI) said it...           0.9132   BJP   \n",
            "1  A working paper from an independent body that ...           0.8999   BJP   \n",
            "2  As Indian Prime Minister Narendra Modi assembl...           0.9829   BJP   \n",
            "3  Narendra Modi's strongman ambitions suffered a...           0.9950   BJP   \n",
            "4  As India concluded the world’s largest electio...           0.9995   BJP   \n",
            "\n",
            "  sentiment_label  \n",
            "0        Positive  \n",
            "1        Positive  \n",
            "2        Positive  \n",
            "3        Positive  \n",
            "4        Positive  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0ced37c5-db3c-4252-8673-119cdec34297\", \"output_full_content.csv\", 649163)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad request URLs saved to 'badrequests.xlsx'\n",
            "Script executed successfully\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install newsapi-python pandas openpyxl requests beautifulsoup4 python-docx vaderSentiment\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import nltk\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "\n",
        "# Download necessary nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize NewsApiClient\n",
        "my_api_key = \"f57a815c8fdb43acacc42aa1f1b814d8\"\n",
        "newsapi = NewsApiClient(api_key=my_api_key)\n",
        "\n",
        "# Function to fetch article content\n",
        "def fetch_article_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_content = '\\n'.join([para.get_text() for para in paragraphs])\n",
        "        return full_content if full_content.strip() != '' else 'removed'\n",
        "    except Exception as e:\n",
        "        return 'removed'\n",
        "\n",
        "# Function to get working URLs\n",
        "def get_working_url(urls):\n",
        "    working_urls = []\n",
        "    bad_requests = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                working_urls.append(url)\n",
        "            else:\n",
        "                bad_requests.append(url)\n",
        "        except Exception as e:\n",
        "            bad_requests.append(url)\n",
        "    return working_urls, bad_requests\n",
        "\n",
        "# Fetch articles\n",
        "print(\"Fetching articles...\")\n",
        "data = newsapi.get_everything(q='indian elections 2024', language='en',page_size=100)\n",
        "\n",
        "if data['status'] != 'ok':\n",
        "    raise Exception(\"Failed to fetch data from News API\")\n",
        "\n",
        "articles = data['articles']\n",
        "df = pd.DataFrame(articles)\n",
        "print(\"Articles fetched successfully.\")\n",
        "\n",
        "# Fetch article content and remove bad URLs\n",
        "print(\"Fetching article content...\")\n",
        "df['full_content'] = df['url'].apply(fetch_article_content)\n",
        "working_urls, bad_requests = get_working_url(df['url'])\n",
        "\n",
        "# Filter out rows with 'removed' content\n",
        "df_filtered = df[df['full_content'] != 'removed']\n",
        "print(f\"Filtered DataFrame has {len(df_filtered)} rows.\")\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# List of political parties to identify in the articles, including common variations\n",
        "party_keywords = {\n",
        "    'BJP': ['BJP', 'Bharatiya Janata Party'],\n",
        "    'INC': ['Congress', 'Indian National Congress', 'INC'],\n",
        "    'AAP': ['AAP', 'Aam Aadmi Party'],\n",
        "    'CPI': ['CPI', 'Communist Party of India'],\n",
        "    'CPM': ['CPM', 'Communist Party of India (Marxist)', 'CPI(M)'],\n",
        "    'NCP': ['NCP', 'Nationalist Congress Party'],\n",
        "    'BSP': ['BSP', 'Bahujan Samaj Party'],\n",
        "    'SP': ['SP', 'Samajwadi Party'],\n",
        "    'RJD': ['RJD', 'Rashtriya Janata Dal'],\n",
        "    'JD(U)': ['JD(U)', 'Janata Dal (United)'],\n",
        "    'TMC': ['TMC', 'All India Trinamool Congress', 'Trinamool Congress'],\n",
        "    'AIADMK': ['AIADMK', 'All India Anna Dravida Munnetra Kazhagam'],\n",
        "    'DMK': ['DMK', 'Dravida Munnetra Kazhagam'],\n",
        "    'Shiv Sena': ['Shiv Sena', 'Shiv Sena'],\n",
        "    'TRS': ['TRS', 'Telangana Rashtra Samithi'],\n",
        "    'YSRCP': ['YSRCP', 'Yuvajana Sramika Rythu Congress Party'],\n",
        "    'TDP': ['TDP', 'Telugu Desam Party'],\n",
        "    'LJP': ['LJP', 'Lok Janshakti Party'],\n",
        "    'RLD': ['RLD', 'Rashtriya Lok Dal'],\n",
        "    'AIMIM': ['AIMIM', 'All India Majlis-e-Ittehadul Muslimeen'],\n",
        "    'JD(S)': ['JD(S)', 'Janata Dal (Secular)'],\n",
        "    'INLD': ['INLD', 'Indian National Lok Dal'],\n",
        "    'JMM': ['JMM', 'Jharkhand Mukti Morcha'],\n",
        "    'SAD': ['SAD', 'Shiromani Akali Dal'],\n",
        "    'RSP': ['RSP', 'Revolutionary Socialist Party'],\n",
        "    'AGP': ['AGP', 'Asom Gana Parishad'],\n",
        "    'BPF': ['BPF', 'Bodoland People\\'s Front'],\n",
        "    'SDF': ['SDF', 'Sikkim Democratic Front'],\n",
        "    'MNDF': ['MNDF', 'Mizo National Front'],\n",
        "    'UDP': ['UDP', 'United Democratic Party (Meghalaya)'],\n",
        "    'NPF': ['NPF', 'Naga People\\'s Front'],\n",
        "    'ZPM': ['ZPM', 'Zoram People\\'s Movement'],\n",
        "    'KC(M)': ['KC(M)', 'Kerala Congress (M)'],\n",
        "    'PDP': ['PDP', 'Peoples Democratic Party'],\n",
        "    'NC': ['NC', 'National Conference']\n",
        "\n",
        "}\n",
        "\n",
        "# Apply sentiment analysis\n",
        "def analyze_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores['compound']\n",
        "\n",
        "def get_sentiment_label(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract party names from text\n",
        "def extract_parties(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    parties_in_text = []\n",
        "    for party, keywords in party_keywords.items():\n",
        "        if any(keyword in entities for keyword in keywords):\n",
        "            parties_in_text.append(party)\n",
        "    return parties_in_text\n",
        "\n",
        "# Function to get sentiment of a text\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']\n",
        "\n",
        "# Function to determine the main party based on weighted sentiment\n",
        "def determine_main_party_weighted(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    party_sentiments = {party: 0 for party in party_keywords}\n",
        "    party_counts = {party: 0 for party in party_keywords}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_text = sentence.text\n",
        "        sentiment = get_sentiment(sentence_text)\n",
        "        parties = extract_parties(sentence_text)\n",
        "\n",
        "        for party in parties:\n",
        "            party_sentiments[party] += sentiment\n",
        "            party_counts[party] += 1\n",
        "\n",
        "    weighted_scores = {party: party_sentiments[party] / party_counts[party] if party_counts[party] > 0 else 0 for party in party_keywords}\n",
        "    max_party_by_weighted_score = max(weighted_scores, key=weighted_scores.get)\n",
        "    max_party_by_count = max(party_counts, key=party_counts.get)\n",
        "\n",
        "    if weighted_scores[max_party_by_weighted_score] != 0:\n",
        "        return max_party_by_weighted_score\n",
        "    else:\n",
        "        return max_party_by_count if party_counts[max_party_by_count] != 0 else 'Unknown'\n",
        "\n",
        "\n",
        "\n",
        "# Ensure all rows are subjected to sentiment analysis\n",
        "df_filtered['sentiment_score'] = df_filtered['full_content'].apply(analyze_sentiment)\n",
        "df_filtered['party'] = df_filtered['full_content'].apply(determine_main_party_weighted)\n",
        "df_filtered['sentiment_label'] = df_filtered['sentiment_score'].apply(get_sentiment_label)\n",
        "\n",
        "# Filter out rows where the party is 'Unknown'\n",
        "df_final = df_filtered[df_filtered['party'] != 'Unknown']\n",
        "\n",
        "# Check the number of rows processed\n",
        "processed_rows = len(df_final)\n",
        "total_rows = len(df_filtered)\n",
        "print(f\"Sentiment analysis completed for {processed_rows} out of {total_rows} rows with identified parties.\")\n",
        "\n",
        "# Save filtered DataFrame to Excel\n",
        "file_path = '/content/output.xlsx'\n",
        "df_final.to_excel(file_path, sheet_name='Sheet1', index=False)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"Excel file found. Attempting to download...\")\n",
        "    files.download(file_path)\n",
        "else:\n",
        "    print(\"Excel file not found.\")\n",
        "\n",
        "# Save filtered DataFrame to CSV\n",
        "csv_file_path = '/content/output_full_content.csv'\n",
        "df_final.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Verify the content of the saved CSV\n",
        "if os.path.exists(csv_file_path):\n",
        "    print(\"CSV file with full content found.\")\n",
        "    df_check = pd.read_csv(csv_file_path)\n",
        "    print(\"Preview of saved CSV:\")\n",
        "    print(df_check.head())  # Display the first few rows for verification\n",
        "    files.download(csv_file_path)\n",
        "else:\n",
        "    print(\"CSV file not found.\")\n",
        "\n",
        "# Save bad request URLs to Excel\n",
        "bad_requests_df = pd.DataFrame({'Bad URLs': bad_requests})\n",
        "bad_requests_file_path = '/content/badrequests.xlsx'\n",
        "bad_requests_df.to_excel(bad_requests_file_path, index=False)\n",
        "\n",
        "if os.path.exists(bad_requests_file_path):\n",
        "    print(\"Bad request URLs saved to 'badrequests.xlsx'\")\n",
        "else:\n",
        "    print(\"Failed to save bad request URLs.\")\n",
        "\n",
        "print(\"Script executed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install newsapi-python pandas openpyxl requests beautifulsoup4 python-docx vaderSentiment\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import nltk\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "\n",
        "# Download necessary nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize NewsApiClient\n",
        "my_api_key = \"f57a815c8fdb43acacc42aa1f1b814d8\"\n",
        "newsapi = NewsApiClient(api_key=my_api_key)\n",
        "\n",
        "# Function to fetch article content\n",
        "def fetch_article_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_content = '\\n'.join([para.get_text() for para in paragraphs])\n",
        "        return full_content if full_content.strip() != '' else 'removed'\n",
        "    except Exception as e:\n",
        "        return 'removed'\n",
        "\n",
        "# Function to get working URLs\n",
        "def get_working_url(urls):\n",
        "    working_urls = []\n",
        "    bad_requests = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                working_urls.append(url)\n",
        "            else:\n",
        "                bad_requests.append(url)\n",
        "        except Exception as e:\n",
        "            bad_requests.append(url)\n",
        "    return working_urls, bad_requests\n",
        "\n",
        "# Fetch articles with a different query\n",
        "print(\"Fetching articles...\")\n",
        "data = newsapi.get_everything(q='Samajwadi Party OR SP OR Bahujan Samaj Party OR BSP OR Communist Party of India OR CPI', language='en', page_size=100)\n",
        "\n",
        "if data['status'] != 'ok':\n",
        "    raise Exception(\"Failed to fetch data from News API\")\n",
        "\n",
        "articles = data['articles']\n",
        "df = pd.DataFrame(articles)\n",
        "print(\"Articles fetched successfully.\")\n",
        "\n",
        "# Fetch article content and remove bad URLs\n",
        "print(\"Fetching article content...\")\n",
        "df['full_content'] = df['url'].apply(fetch_article_content)\n",
        "working_urls, bad_requests = get_working_url(df['url'])\n",
        "\n",
        "# Filter out rows with 'removed' content\n",
        "df_filtered = df[df['full_content'] != 'removed']\n",
        "print(f\"Filtered DataFrame has {len(df_filtered)} rows.\")\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# List of political parties to identify in the articles, including common variations\n",
        "party_keywords = {\n",
        "      'BJP': ['BJP', 'Bharatiya Janata Party'],\n",
        "    'INC': ['Congress', 'Indian National Congress', 'INC'],\n",
        "    'AAP': ['AAP', 'Aam Aadmi Party'],\n",
        "    'CPI': ['CPI', 'Communist Party of India'],\n",
        "    'CPM': ['CPM', 'Communist Party of India (Marxist)', 'CPI(M)'],\n",
        "    'NCP': ['NCP', 'Nationalist Congress Party'],\n",
        "    'BSP': ['BSP', 'Bahujan Samaj Party'],\n",
        "    'SP': ['SP', 'Samajwadi Party'],\n",
        "    'RJD': ['RJD', 'Rashtriya Janata Dal'],\n",
        "    'JD(U)': ['JD(U)', 'Janata Dal (United)'],\n",
        "    'TMC': ['TMC', 'All India Trinamool Congress', 'Trinamool Congress'],\n",
        "    'AIADMK': ['AIADMK', 'All India Anna Dravida Munnetra Kazhagam'],\n",
        "    'DMK': ['DMK', 'Dravida Munnetra Kazhagam'],\n",
        "    'Shiv Sena': ['Shiv Sena', 'Shiv Sena'],\n",
        "    'TRS': ['TRS', 'Telangana Rashtra Samithi'],\n",
        "    'YSRCP': ['YSRCP', 'Yuvajana Sramika Rythu Congress Party'],\n",
        "    'TDP': ['TDP', 'Telugu Desam Party'],\n",
        "    'LJP': ['LJP', 'Lok Janshakti Party'],\n",
        "    'RLD': ['RLD', 'Rashtriya Lok Dal'],\n",
        "    'AIMIM': ['AIMIM', 'All India Majlis-e-Ittehadul Muslimeen'],\n",
        "    'JD(S)': ['JD(S)', 'Janata Dal (Secular)'],\n",
        "    'INLD': ['INLD', 'Indian National Lok Dal'],\n",
        "    'JMM': ['JMM', 'Jharkhand Mukti Morcha'],\n",
        "    'SAD': ['SAD', 'Shiromani Akali Dal'],\n",
        "    'RSP': ['RSP', 'Revolutionary Socialist Party'],\n",
        "    'AGP': ['AGP', 'Asom Gana Parishad'],\n",
        "    'BPF': ['BPF', 'Bodoland People\\'s Front'],\n",
        "    'SDF': ['SDF', 'Sikkim Democratic Front'],\n",
        "    'MNDF': ['MNDF', 'Mizo National Front'],\n",
        "    'UDP': ['UDP', 'United Democratic Party (Meghalaya)'],\n",
        "    'NPF': ['NPF', 'Naga People\\'s Front'],\n",
        "    'ZPM': ['ZPM', 'Zoram People\\'s Movement'],\n",
        "    'KC(M)': ['KC(M)', 'Kerala Congress (M)'],\n",
        "    'PDP': ['PDP', 'Peoples Democratic Party'],\n",
        "    'NC': ['NC', 'National Conference']\n",
        "}\n",
        "\n",
        "# Apply sentiment analysis\n",
        "def analyze_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores['compound']\n",
        "\n",
        "def get_sentiment_label(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract party names from text\n",
        "def extract_parties(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    parties_in_text = []\n",
        "    for party, keywords in party_keywords.items():\n",
        "        if any(keyword in entities for keyword in keywords):\n",
        "            parties_in_text.append(party)\n",
        "    return parties_in_text\n",
        "\n",
        "# Function to get sentiment of a text\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']\n",
        "\n",
        "# Function to determine the main party based on weighted sentiment\n",
        "def determine_main_party_weighted(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    party_sentiments = {party: 0 for party in party_keywords}\n",
        "    party_counts = {party: 0 for party in party_keywords}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_text = sentence.text\n",
        "        sentiment = get_sentiment(sentence_text)\n",
        "        parties = extract_parties(sentence_text)\n",
        "\n",
        "        for party in parties:\n",
        "            party_sentiments[party] += sentiment\n",
        "            party_counts[party] += 1\n",
        "\n",
        "    weighted_scores = {party: party_sentiments[party] / party_counts[party] if party_counts[party] > 0 else 0 for party in party_keywords}\n",
        "    max_party_by_weighted_score = max(weighted_scores, key=weighted_scores.get)\n",
        "    max_party_by_count = max(party_counts, key=party_counts.get)\n",
        "\n",
        "    if weighted_scores[max_party_by_weighted_score] != 0:\n",
        "        return max_party_by_weighted_score\n",
        "    else:\n",
        "        return max_party_by_count if party_counts[max_party_by_count] != 0 else 'Unknown'\n",
        "\n",
        "\n",
        "\n",
        "# Ensure all rows are subjected to sentiment analysis\n",
        "df_filtered['sentiment_score'] = df_filtered['full_content'].apply(analyze_sentiment)\n",
        "df_filtered['party'] = df_filtered['full_content'].apply(determine_main_party_weighted)\n",
        "df_filtered['sentiment_label'] = df_filtered['sentiment_score'].apply(get_sentiment_label)\n",
        "\n",
        "# Filter out rows where the party is 'Unknown'\n",
        "df_final = df_filtered[df_filtered['party'] != 'Unknown']\n",
        "\n",
        "# Check the number of rows processed\n",
        "processed_rows = len(df_final)\n",
        "total_rows = len(df_filtered)\n",
        "print(f\"Sentiment analysis completed for {processed_rows} out of {total_rows} rows with identified parties.\")\n",
        "\n",
        "# Save filtered DataFrame to Excel\n",
        "file_path = '/content/output_new.xlsx'\n",
        "df_final.to_excel(file_path, sheet_name='Sheet1', index=False)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"Excel file found. Attempting to download...\")\n",
        "    files.download(file_path)\n",
        "else:\n",
        "    print(\"Excel file not found.\")\n",
        "\n",
        "# Save filtered DataFrame to CSV\n",
        "csv_file_path = '/content/output_new_full_content.csv'\n",
        "df_final.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Verify the content of the saved CSV\n",
        "if os.path.exists(csv_file_path):\n",
        "    print(\"CSV file with full content found.\")\n",
        "    df_check = pd.read_csv(csv_file_path)\n",
        "    print(\"Preview of saved CSV:\")\n",
        "    print(df_check.head())  # Display the first few rows for verification\n",
        "    files.download(csv_file_path)\n",
        "else:\n",
        "    print(\"CSV file not found.\")\n",
        "\n",
        "# Save bad request URLs to Excel\n",
        "bad_requests_df = pd.DataFrame({'Bad URLs': bad_requests})\n",
        "bad_requests_file_path = '/content/badrequests_new.xlsx'\n",
        "bad_requests_df.to_excel(bad_requests_file_path, index=False)\n",
        "\n",
        "if os.path.exists(bad_requests_file_path):\n",
        "    print(\"Bad request URLs saved to 'badrequests_new.xlsx'\")\n",
        "else:\n",
        "    print(\"Failed to save bad request URLs.\")\n",
        "\n",
        "print(\"Script executed successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "08mc1W21Npbv",
        "outputId": "3230a43a-7479-4f44-8db6-15d3577269c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles...\n",
            "Articles fetched successfully.\n",
            "Fetching article content...\n",
            "Filtered DataFrame has 97 rows.\n",
            "                                        article_text party  sentiment\n",
            "0  The BJP has launched its new campaign for the ...  None    -0.2263\n",
            "1   AAP's manifesto focuses on health and education.  None     0.0000\n",
            "Sentiment analysis completed for 72 out of 97 rows with identified parties.\n",
            "Excel file found. Attempting to download...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_acfa6490-f266-40ab-989d-e7b7f4c6b264\", \"output_new.xlsx\", 128307)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file with full content found.\n",
            "Preview of saved CSV:\n",
            "                                              source            author  \\\n",
            "0  {'id': 'al-jazeera-english', 'name': 'Al Jazee...            AJLabs   \n",
            "1  {'id': 'al-jazeera-english', 'name': 'Al Jazee...  Al Jazeera Staff   \n",
            "2  {'id': 'al-jazeera-english', 'name': 'Al Jazee...       Saif Khalid   \n",
            "3  {'id': 'al-jazeera-english', 'name': 'Al Jazee...  Al Jazeera Staff   \n",
            "4  {'id': 'al-jazeera-english', 'name': 'Al Jazee...  Al Jazeera Staff   \n",
            "\n",
            "                                               title  \\\n",
            "0     Mapping the results of the India election 2024   \n",
            "1  India election results: Big wins, losses and s...   \n",
            "2  India election results: Did ‘secular’ parties ...   \n",
            "3  India Lok Sabha election 2024 Phase 7: Who vot...   \n",
            "4  India Lok Sabha election 2024 Phase 6: Who vot...   \n",
            "\n",
            "                                         description  \\\n",
            "0  The Bharatiya Janata Party, together with its ...   \n",
            "1  A tight Varanasi race and BJP's Maharashtra do...   \n",
            "2  Scared of the BJP describing them as pro-Musli...   \n",
            "3  Voters will decide the fate of 904 candidates ...   \n",
            "4  Voters from eight states and union territories...   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.aljazeera.com/news/2024/6/6/mappin...   \n",
            "1  https://www.aljazeera.com/news/2024/6/4/india-...   \n",
            "2  https://www.aljazeera.com/news/2024/6/5/india-...   \n",
            "3  https://www.aljazeera.com/news/2024/5/31/india...   \n",
            "4  https://www.aljazeera.com/news/2024/5/24/india...   \n",
            "\n",
            "                                          urlToImage           publishedAt  \\\n",
            "0  https://www.aljazeera.com/wp-content/uploads/2...  2024-06-06T04:31:45Z   \n",
            "1  https://www.aljazeera.com/wp-content/uploads/2...  2024-06-04T11:49:26Z   \n",
            "2  https://www.aljazeera.com/wp-content/uploads/2...  2024-06-05T12:04:02Z   \n",
            "3  https://www.aljazeera.com/wp-content/uploads/2...  2024-05-31T07:41:15Z   \n",
            "4  https://www.aljazeera.com/wp-content/uploads/2...  2024-05-24T07:13:20Z   \n",
            "\n",
            "                                             content  \\\n",
            "0  With all of Indias 640 million votes counted f...   \n",
            "1  As Indias election results become clearer, wit...   \n",
            "2  As Indian opposition leader Rahul Gandhi addre...   \n",
            "3  Indians will cast ballots in the last phase of...   \n",
            "4  Indias staggered general election is heading t...   \n",
            "\n",
            "                                        full_content  sentiment_score  \\\n",
            "0  The Bharatiya Janata Party has fallen short of...           0.9982   \n",
            "1  A tight Varanasi race and BJP’s Maharashtra do...           0.9995   \n",
            "2  Scared of the BJP describing them as pro-Musli...           0.9985   \n",
            "3  Voters will decide the fate of 904 candidates ...           0.9951   \n",
            "4  Voters from eight states and union territories...           0.9961   \n",
            "\n",
            "  sentiment_label party  \n",
            "0        Positive   BJP  \n",
            "1        Positive   BJP  \n",
            "2        Positive   BJP  \n",
            "3        Positive   BJP  \n",
            "4        Positive   BJP  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6701b765-e907-461b-b4e7-56f09ae29037\", \"output_new_full_content.csv\", 374686)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad request URLs saved to 'badrequests_new.xlsx'\n",
            "Script executed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newsapi-python pandas openpyxl requests beautifulsoup4 python-docx vaderSentiment spacy\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import nltk\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import spacy\n",
        "\n",
        "# Download necessary nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize NewsApiClient\n",
        "my_api_key = \"f57a815c8fdb43acacc42aa1f1b814d8\"\n",
        "newsapi = NewsApiClient(api_key=my_api_key)\n",
        "\n",
        "# Function to fetch article content\n",
        "def fetch_article_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_content = '\\n'.join([para.get_text() for para in paragraphs])\n",
        "        return full_content if full_content.strip() != '' else 'removed'\n",
        "    except Exception as e:\n",
        "        return 'removed'\n",
        "\n",
        "# Function to get working URLs\n",
        "def get_working_url(urls):\n",
        "    working_urls = []\n",
        "    bad_requests = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                working_urls.append(url)\n",
        "            else:\n",
        "                bad_requests.append(url)\n",
        "        except Exception as e:\n",
        "            bad_requests.append(url)\n",
        "    return working_urls, bad_requests\n",
        "\n",
        "# Fetch articles\n",
        "print(\"Fetching articles...\")\n",
        "data = newsapi.get_everything(q='indian elections 2024', language='en')\n",
        "\n",
        "if data['status'] != 'ok':\n",
        "    raise Exception(\"Failed to fetch data from News API\")\n",
        "\n",
        "articles = data['articles']\n",
        "df = pd.DataFrame(articles)\n",
        "print(\"Articles fetched successfully.\")\n",
        "\n",
        "# Fetch article content and remove bad URLs\n",
        "print(\"Fetching article content...\")\n",
        "df['full_content'] = df['url'].apply(fetch_article_content)\n",
        "working_urls, bad_requests = get_working_url(df['url'])\n",
        "\n",
        "# Filter out rows with 'removed' content\n",
        "df_filtered = df[df['full_content'] != 'removed']\n",
        "print(f\"Filtered DataFrame has {len(df_filtered)} rows.\")\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# List of political parties to identify in the articles, including common variations\n",
        "party_keywords = {\n",
        "    'BJP': ['BJP', 'Bharatiya Janata Party'],\n",
        "    'INC': ['Congress', 'Indian National Congress', 'INC'],\n",
        "    'AAP': ['AAP', 'Aam Aadmi Party'],\n",
        "    'CPI': ['CPI', 'Communist Party of India'],\n",
        "    'CPM': ['CPM', 'Communist Party of India (Marxist)', 'CPI(M)'],\n",
        "    'NCP': ['NCP', 'Nationalist Congress Party'],\n",
        "    'BSP': ['BSP', 'Bahujan Samaj Party'],\n",
        "    'SP': ['SP', 'Samajwadi Party'],\n",
        "    'RJD': ['RJD', 'Rashtriya Janata Dal'],\n",
        "    'JD(U)': ['JD(U)', 'Janata Dal (United)'],\n",
        "    'TMC': ['TMC', 'All India Trinamool Congress', 'Trinamool Congress'],\n",
        "    'AIADMK': ['AIADMK', 'All India Anna Dravida Munnetra Kazhagam'],\n",
        "    'DMK': ['DMK', 'Dravida Munnetra Kazhagam'],\n",
        "    'Shiv Sena': ['Shiv Sena', 'Shiv Sena'],\n",
        "    'TRS': ['TRS', 'Telangana Rashtra Samithi'],\n",
        "    'YSRCP': ['YSRCP', 'Yuvajana Sramika Rythu Congress Party'],\n",
        "    'TDP': ['TDP', 'Telugu Desam Party'],\n",
        "    'LJP': ['LJP', 'Lok Janshakti Party'],\n",
        "    'RLD': ['RLD', 'Rashtriya Lok Dal'],\n",
        "    'AIMIM': ['AIMIM', 'All India Majlis-e-Ittehadul Muslimeen'],\n",
        "    'JD(S)': ['JD(S)', 'Janata Dal (Secular)'],\n",
        "    'INLD': ['INLD', 'Indian National Lok Dal'],\n",
        "    'JMM': ['JMM', 'Jharkhand Mukti Morcha'],\n",
        "    'SAD': ['SAD', 'Shiromani Akali Dal'],\n",
        "    'RSP': ['RSP', 'Revolutionary Socialist Party'],\n",
        "    'AGP': ['AGP', 'Asom Gana Parishad'],\n",
        "    'BPF': ['BPF', 'Bodoland People\\'s Front'],\n",
        "    'SDF': ['SDF', 'Sikkim Democratic Front'],\n",
        "    'MNDF': ['MNDF', 'Mizo National Front'],\n",
        "    'UDP': ['UDP', 'United Democratic Party (Meghalaya)'],\n",
        "    'NPF': ['NPF', 'Naga People\\'s Front'],\n",
        "    'ZPM': ['ZPM', 'Zoram People\\'s Movement'],\n",
        "    'KC(M)': ['KC(M)', 'Kerala Congress (M)'],\n",
        "    'PDP': ['PDP', 'Peoples Democratic Party'],\n",
        "    'NC': ['NC', 'National Conference']\n",
        "}\n",
        "\n",
        "# Function to extract party names from text\n",
        "def extract_parties(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    parties_in_text = []\n",
        "    for party, keywords in party_keywords.items():\n",
        "        if any(keyword in entities for keyword in keywords):\n",
        "            parties_in_text.append(party)\n",
        "    return parties_in_text\n",
        "\n",
        "# Function to get sentiment of a text\n",
        "def get_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']\n",
        "\n",
        "# Function to determine the main party based on weighted sentiment\n",
        "def determine_main_party_weighted(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    party_sentiments = {party: 0 for party in party_keywords}\n",
        "    party_counts = {party: 0 for party in party_keywords}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_text = sentence.text\n",
        "        sentiment = get_sentiment(sentence_text)\n",
        "        parties = extract_parties(sentence_text)\n",
        "\n",
        "        for party in parties:\n",
        "            party_sentiments[party] += sentiment\n",
        "            party_counts[party] += 1\n",
        "\n",
        "    weighted_scores = {party: party_sentiments[party] / party_counts[party] if party_counts[party] > 0 else 0 for party in party_keywords}\n",
        "    max_party_by_weighted_score = max(weighted_scores, key=weighted_scores.get)\n",
        "    max_party_by_count = max(party_counts, key=party_counts.get)\n",
        "\n",
        "    if weighted_scores[max_party_by_weighted_score] != 0:\n",
        "        return max_party_by_weighted_score\n",
        "    else:\n",
        "        return max_party_by_count if party_counts[max_party_by_count] != 0 else 'Unknown'\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df_filtered['sentiment_score'] = df_filtered['full_content'].apply(analyze_sentiment)\n",
        "df_filtered['party'] = df_filtered['full_content'].apply(determine_main_party_weighted)\n",
        "df_filtered['sentiment_label'] = df_filtered['sentiment_score'].apply(get_sentiment_label)\n",
        "\n",
        "# Filter out rows where the party is 'Unknown'\n",
        "df_final = df_filtered[df_filtered['party'] != 'Unknown']\n",
        "\n",
        "# Check the number of rows processed\n",
        "processed_rows = len(df_final)\n",
        "total_rows = len(df_filtered)\n",
        "print(f\"Sentiment analysis completed for {processed_rows} out of {total_rows} rows with identified parties.\")\n",
        "\n",
        "# Save filtered DataFrame to Excel\n",
        "file_path = '/content/output.xlsx'\n",
        "df_final.to_excel(file_path, sheet_name='Sheet1', index=False)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"Excel file found. Attempting to download...\")\n",
        "    files.download(file_path)\n",
        "else:\n",
        "    print(\"Excel file not found.\")\n",
        "\n",
        "# Save filtered DataFrame to CSV\n",
        "csv_file_path = '/content/output_full_content.csv'\n",
        "df_final.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Verify the content of the saved CSV\n",
        "if os.path.exists(csv_file_path):\n",
        "    print(\"CSV file with full content found.\")\n",
        "    df_check = pd.read_csv(csv_file_path)\n",
        "    print(\"Preview of saved CSV:\")\n",
        "    print(df_check.head())  # Display the first few rows for verification\n",
        "    files.download(csv_file_path)\n",
        "else:\n",
        "    print(\"CSV file not found.\")\n",
        "\n",
        "# Save bad request URLs to Excel\n",
        "bad_requests_df = pd.DataFrame({'Bad URLs': bad_requests})\n",
        "bad_requests_file_path = '/content/badrequests.xlsx'\n",
        "bad_requests_df.to_excel(bad_requests_file_path, index=False)\n",
        "\n",
        "if os.path.exists(bad_requests_file_path):\n",
        "    print(\"Bad request URLs saved to 'badrequests.xlsx'\")\n",
        "else:\n",
        "    print(\"Failed to save bad request URLs.\")\n",
        "\n",
        "print(\"Script executed successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qsu6v-OV6OYX",
        "outputId": "76b28e05-152b-4a32-b35a-f1f7130025eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles...\n",
            "Articles fetched successfully.\n",
            "Fetching article content...\n",
            "Filtered DataFrame has 85 rows.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-65b2ddd924e7>:155: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['sentiment_score'] = df_filtered['full_content'].apply(analyze_sentiment)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis completed for 58 out of 85 rows with identified parties.\n",
            "Excel file found. Attempting to download...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-65b2ddd924e7>:156: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['party'] = df_filtered['full_content'].apply(determine_main_party_weighted)\n",
            "<ipython-input-3-65b2ddd924e7>:157: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['sentiment_label'] = df_filtered['sentiment_score'].apply(get_sentiment_label)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f57a071e-a205-4afd-ba8a-a5a713bb12c6\", \"output.xlsx\", 226064)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file with full content found.\n",
            "Preview of saved CSV:\n",
            "                                              source  \\\n",
            "0  {'id': 'business-insider', 'name': 'Business I...   \n",
            "1                     {'id': 'time', 'name': 'Time'}   \n",
            "2  {'id': 'business-insider', 'name': 'Business I...   \n",
            "3  {'id': 'business-insider', 'name': 'Business I...   \n",
            "4        {'id': None, 'name': 'Yahoo Entertainment'}   \n",
            "\n",
            "                                              author  \\\n",
            "0                                     Rebecca Rommen   \n",
            "1                                    Astha Rajvanshi   \n",
            "2                                        Matthew Loh   \n",
            "3                                         Tom Porter   \n",
            "4  Vandinika Shukla, Harvard Kennedy School and B...   \n",
            "\n",
            "                                               title  \\\n",
            "0  Indian authorities seize over $1 billion worth...   \n",
            "1  The Controversy Over a New Population Study Fr...   \n",
            "2  It's been 3 days since Modi won, and we're alr...   \n",
            "3  The competition between India and China is abo...   \n",
            "4  Indian election was awash in deepfakes – but A...   \n",
            "\n",
            "                                         description  \\\n",
            "0  The winner of India's general election, which ...   \n",
            "1  Critics say the study could sow communal disco...   \n",
            "2  Modi's next term as prime minister is set to l...   \n",
            "3  India and China are locked in a battle between...   \n",
            "4  Campaigns used deepfakes to connect with voter...   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.businessinsider.com/indian-authori...   \n",
            "1  https://time.com/6976854/india-population-stud...   \n",
            "2  https://www.businessinsider.com/narendra-modi-...   \n",
            "3  https://www.businessinsider.com/india-china-co...   \n",
            "4  https://www.yahoo.com/news/indian-election-awa...   \n",
            "\n",
            "                                          urlToImage           publishedAt  \\\n",
            "0  https://i.insider.com/6649a39d20abc1efe8fb6473...  2024-05-19T13:22:38Z   \n",
            "1  https://api.time.com/wp-content/uploads/2024/0...  2024-05-10T15:16:47Z   \n",
            "2  https://i.insider.com/666286961cd3b17790443e58...  2024-06-07T05:59:02Z   \n",
            "3  https://i.insider.com/66631e09cc442a2f676f130d...  2024-06-08T10:03:17Z   \n",
            "4  https://s.yimg.com/ny/api/res/1.2/JhjdV20_cCeY...  2024-06-10T13:43:43Z   \n",
            "\n",
            "                                             content  \\\n",
            "0  Election duty staff cast their votes on May 13...   \n",
            "1  A working paper from an independent body that ...   \n",
            "2  India's Prime Minister Narendra Modi addresses...   \n",
            "3  Jack Taylor/Getty Images; Carlos Barria/Pool/A...   \n",
            "4  As India concluded the worlds largest election...   \n",
            "\n",
            "                                        full_content  sentiment_score party  \\\n",
            "0  The Election Commission of India (ECI) said it...           0.9132   BJP   \n",
            "1  A working paper from an independent body that ...           0.8999   BJP   \n",
            "2  As Indian Prime Minister Narendra Modi assembl...           0.9829   BJP   \n",
            "3  Narendra Modi's strongman ambitions suffered a...           0.9950   BJP   \n",
            "4  As India concluded the world’s largest electio...           0.9995   BJP   \n",
            "\n",
            "  sentiment_label  \n",
            "0        Positive  \n",
            "1        Positive  \n",
            "2        Positive  \n",
            "3        Positive  \n",
            "4        Positive  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0b9e2c8d-65a2-4f74-98bb-41e606b9a19d\", \"output_full_content.csv\", 634227)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad request URLs saved to 'badrequests.xlsx'\n",
            "Script executed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LIO2MgTn6PiH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}